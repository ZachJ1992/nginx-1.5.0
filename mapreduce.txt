    MapReduce基础
至今为止，解决大数据问题的可行方法(feasible approach)是分治法(devide an conque),一个很基本的计算机科学概念，一般都在本科的时候就介绍过。 基本的思想是将大问题分成很多小的子问题。

在一定程度上，子问题是独立的，他们可以被不同的worker并行解决--(同一个处理核心的线程)， 每个多核处理器的不同核里解决， 以及同一机器的多个处理器中解决， 或者集群中的多台机器来解决。 
从每个单独的worker返回的中间结果然后被合并到最终的输出结果中。

分治法算法后面的基本原则适合很广泛的不同应用领域。 然而， 它们的实现各不相同，也具有不同的复杂度。例如， 下面就是一些需要定位的问题:
1) 如何将大问题切分为多个小问题? 更加特别的， 如何分解问题，以便那些小能并行执行?
2) 如何为可能分布在大量机器上的workers分配任务? (记住一点有些workers比其他的可能更适合运行某些任务，比如，由于可用资源，地理位置限制等等)
3) 我们如何确保workers获取到它们需要的数据?
4) 我们如何协调不同的worker之间的同步?
5) 我们如何从一个worker共享部分另外worker需要的结果?
6) 我们如何在软件错误和硬件问题的情况下完成上述所有的问题?

在传统的并行或者分布式编程环境， 开发者需要明确的定址上述很多问题(有时候，所有问题都要解决)。 在共享内存编程下， 开发者需要明确协调对共享数据结构的访问，通过同步原语， 比如互斥锁，来明确处理进程同步通过诸如壁垒这样的设备， 需要不断提高一般问题的警觉，比如死锁以及竞争条件。 语言扩展，就像共享内存并行的OpenMP， 或者实现集群级别的并行消息传递接口的库(MPI message passing interface), 提供逻辑抽象，隐藏了操作系统同步和通信原语。然而， 即使使用这些扩展， 开发者也仍然背负着资源是如何让worker可用的职责。 另外， 这些框架极大的设计于解决处理器密集型问题， 


从数据放置的角度看传统大规模数据处理中的问题
在传统集群架构中(例如HPC高性能计算)中，计算与存储是两个分离的组件。 虽然不同系统的具体实现有差异，但总体思路是一致的: 计算节点从存储节点将数据读入，处理数据，将结果写出。

随着数据量的增大，数据处理对于计算能力的要求也在提高。 随着计算能力的提高， 存储节点与计算节点间的连接带宽渐渐成为整个系统性能的瓶颈。 解决这个问题， 一种办法是购买更高级的网络设备， 提高宽带。 这往往是很昂贵的，因为随着传输速率的增长， 网络设备价格增长高于线性: 10倍速度的设备往往要价不止10倍。

另一种方法是避免计算和存储的分离。

MapReduce底层的分布式文件系统(DFS Distributed File System)采用的正是后者。 Google自家的MapReduce实现采用Google分布式文件系统GFS(Google File System);而开源的hadoop分布式文件系统HDFS(Hadoop Distributed File System).

虽然没有DFS MapReduce也可以工作， 但将丧失一大优势(计算与存储结合)。

GFS与HDFS
DFS在MapReduce诞生之时并不算新鲜事物，但MapReduce使用的DFS(例如GFS, HDFS)在前人工作的基础上进行了改进，使之更适应进行大规模数据处理。 
DFS的核心思想是数据分块与复制。数据分块并不是新概念， 但对于本地磁盘几KB的块(block)， DFS中的块要大得多(通常默认为64M). DFS采用主从架构(master-slave architecture)， master维护文件表(元数据、文件结构、文件-块映射、块地址、访问控制信息等)，slave存储实际文件数据。 GFS中master称为GFS master, slave称为块服务器(chunkserver). HDFS中master和slave分别被称为名字节点(namenode)与数据节点(datanode).
